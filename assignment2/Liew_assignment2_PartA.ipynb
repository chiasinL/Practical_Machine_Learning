{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a990f1eb",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Name        : Chia Sin Liew   \n",
    "Last edited : March 31st, 2022 \n",
    "\n",
    "The goal of this assignment is to solve regression and classification problems using following models.\n",
    "\n",
    "- **Part A**: Regression Problem – Linear Regression using the Stochastic Gradient Descent algorithm\n",
    "- **Part B**: Classification Problem – K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0e712",
   "metadata": {},
   "source": [
    "## Part A: Linear Regression using SGD\n",
    "\n",
    "You will perform regression on the following dataset using the Polynomial Regression Model with the Stochastic Gradient Descent (SGD) algorithm (use Scikit-Learn implementation). \n",
    "\n",
    "Your goal is to:\n",
    "\n",
    "- Minimize the Test dataset’s Mean Squared Error\n",
    "- Maximize the Test dataset’s Coefficient of determination R2 variance score\n",
    "\n",
    "### **Dataset**:\n",
    "The energy efficiency dataset *EnergyEfficiency.xlsx* is created to perform energy analysis. The dataset comprises 768 samples and 8 features (X1 to X8). It has two real valued target variables (Y1 and Y2), i.e., heating load and cooling load, respectively.\n",
    "\n",
    "- X1: Relative Compactness\n",
    "- X2: Surface Area\n",
    "- X3: Wall Area\n",
    "- X4: Roof Area\n",
    "- X5: Overall Height\n",
    "- X6: Orientation\n",
    "- X7: Glazing Area\n",
    "- X8: Glazing Area Distribution\n",
    "- Y1: Heating Load\n",
    "- Y2: Cooling Load\n",
    "\n",
    "For this task you will **only predict the heating load**. Thus, use **Y1** as the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c6ac9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys \n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad5320",
   "metadata": {},
   "source": [
    "### **Pre-Processing**:\n",
    "\n",
    "- Load the .xlsx file as a Pandas DataFrame object. You may use pandas read_excel() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8bc474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>17.88</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.54</td>\n",
       "      <td>16.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.44</td>\n",
       "      <td>17.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.48</td>\n",
       "      <td>16.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.64</td>\n",
       "      <td>16.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
       "0    0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
       "1    0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
       "2    0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
       "3    0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
       "4    0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n",
       "..    ...    ...    ...     ...  ...  ..  ...  ..    ...    ...\n",
       "763  0.64  784.0  343.0  220.50  3.5   5  0.4   5  17.88  21.40\n",
       "764  0.62  808.5  367.5  220.50  3.5   2  0.4   5  16.54  16.88\n",
       "765  0.62  808.5  367.5  220.50  3.5   3  0.4   5  16.44  17.11\n",
       "766  0.62  808.5  367.5  220.50  3.5   4  0.4   5  16.48  16.61\n",
       "767  0.62  808.5  367.5  220.50  3.5   5  0.4   5  16.64  16.03\n",
       "\n",
       "[768 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Dataset-Assignment2/EnergyEfficiency.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866db988",
   "metadata": {},
   "source": [
    "- Create a separate feature set (Data Matrix X) and target (1D Array y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b2eacaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data frame for features(X): \n",
      "      X1     X2     X3      X4   X5  X6   X7  X8\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0\n",
      "\n",
      "Dimension of X:  (768, 8)\n",
      "\n",
      "Data frame for target(Y): \n",
      "         Y1\n",
      "0    15.55\n",
      "1    15.55\n",
      "2    15.55\n",
      "3    15.55\n",
      "4    20.84\n",
      "..     ...\n",
      "763  17.88\n",
      "764  16.54\n",
      "765  16.44\n",
      "766  16.48\n",
      "767  16.64\n",
      "\n",
      "[768 rows x 1 columns]\n",
      "\n",
      "Dimension of Y:  (768, 1)\n",
      "\n",
      "X data type:  float64\n",
      "Y data type:  float64\n"
     ]
    }
   ],
   "source": [
    "def create_feature_target_arrays(df, target, drop_columns):\n",
    "    # y for target\n",
    "    y = df[target].to_frame()\n",
    "    # X for features\n",
    "    X = df.drop(columns=drop_columns)\n",
    "\n",
    "    print(\"\\nData frame for features(X): \\n\", X.head())\n",
    "    print(\"\\nDimension of X: \", X.shape)\n",
    "    print(\"\\nData frame for target(Y): \\n\", y)\n",
    "    print(\"\\nDimension of Y: \", y.shape)\n",
    "\n",
    "    # convert into np arrays\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y).ravel()\n",
    "\n",
    "    print(\"\\nX data type: \", X.dtype)\n",
    "    print(\"Y data type: \", y.dtype)\n",
    "    \n",
    "    return(X, y)\n",
    "\n",
    "\n",
    "X, y = create_feature_target_arrays(df, \"Y1\", [\"Y1\", \"Y2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e4e72",
   "metadata": {},
   "source": [
    "- Partition the data in training & test subsets (80% - 20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7931eef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 8)\n",
      "(154, 8)\n",
      "(614,)\n",
      "(154,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1269c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86</td>\n",
       "      <td>588.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.71</td>\n",
       "      <td>710.5</td>\n",
       "      <td>269.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.74</td>\n",
       "      <td>686.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.69</td>\n",
       "      <td>735.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1     X2     X3      X4   X5   X6    X7   X8     Y1\n",
       "0    0.86  588.0  294.0  147.00  7.0  5.0  0.40  4.0  32.75\n",
       "1    0.71  710.5  269.5  220.50  3.5  2.0  0.25  1.0  12.57\n",
       "2    0.62  808.5  367.5  220.50  3.5  5.0  0.40  3.0  16.74\n",
       "3    0.98  514.5  294.0  110.25  7.0  2.0  0.10  3.0  24.28\n",
       "4    0.98  514.5  294.0  110.25  7.0  3.0  0.25  4.0  28.55\n",
       "..    ...    ...    ...     ...  ...  ...   ...  ...    ...\n",
       "609  0.64  784.0  343.0  220.50  3.5  3.0  0.40  4.0  19.13\n",
       "610  0.74  686.0  245.0  220.50  3.5  2.0  0.10  3.0  10.39\n",
       "611  0.64  784.0  343.0  220.50  3.5  3.0  0.10  3.0  15.36\n",
       "612  0.69  735.0  294.0  220.50  3.5  5.0  0.00  0.0   6.81\n",
       "613  0.79  637.0  343.0  147.00  7.0  3.0  0.40  3.0  41.30\n",
       "\n",
       "[614 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a data frame for feature selection through correlation analysis\n",
    "df_train = pd.DataFrame(np.concatenate((X_train, np.reshape(y_train, (614, 1))), axis=1), \n",
    "                      columns=['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y1'])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a00bb3",
   "metadata": {},
   "source": [
    "- Feature Selection: For optimal test performance, you may use a subset of features. However, it’s up to you to decide which features to select or to keep all\n",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03604f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1    1.000000\n",
      "X5    0.889144\n",
      "X1    0.632955\n",
      "X3    0.425035\n",
      "X7    0.248738\n",
      "X8    0.087487\n",
      "X6    0.007846\n",
      "X2   -0.667584\n",
      "X4   -0.860543\n",
      "Name: Y1, dtype: float64\n",
      "\n",
      "Keep:\n",
      "\n",
      "Y1    1.000000\n",
      "X5    0.889144\n",
      "X1    0.632955\n",
      "X3    0.425035\n",
      "X7    0.248738\n",
      "X2   -0.667584\n",
      "X4   -0.860543\n",
      "Name: Y1, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X7</th>\n",
       "      <th>Y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86</td>\n",
       "      <td>588.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>32.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.71</td>\n",
       "      <td>710.5</td>\n",
       "      <td>269.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>12.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>16.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>24.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>28.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>19.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.74</td>\n",
       "      <td>686.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>15.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.69</td>\n",
       "      <td>735.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>41.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1     X2     X3      X4   X5    X7     Y1\n",
       "0    0.86  588.0  294.0  147.00  7.0  0.40  32.75\n",
       "1    0.71  710.5  269.5  220.50  3.5  0.25  12.57\n",
       "2    0.62  808.5  367.5  220.50  3.5  0.40  16.74\n",
       "3    0.98  514.5  294.0  110.25  7.0  0.10  24.28\n",
       "4    0.98  514.5  294.0  110.25  7.0  0.25  28.55\n",
       "..    ...    ...    ...     ...  ...   ...    ...\n",
       "609  0.64  784.0  343.0  220.50  3.5  0.40  19.13\n",
       "610  0.74  686.0  245.0  220.50  3.5  0.10  10.39\n",
       "611  0.64  784.0  343.0  220.50  3.5  0.10  15.36\n",
       "612  0.69  735.0  294.0  220.50  3.5  0.00   6.81\n",
       "613  0.79  637.0  343.0  147.00  7.0  0.40  41.30\n",
       "\n",
       "[614 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate Pearson's correlation\n",
    "corr_threshold = 0.2\n",
    "\n",
    "feature_corr = df_train.corr()[\"Y1\"].sort_values(ascending=False)\n",
    "print(feature_corr)\n",
    "\n",
    "# Keep features above the selected threshold\n",
    "print(\"\\nKeep:\\n\")\n",
    "retained_features = feature_corr[feature_corr.abs() >= corr_threshold]\n",
    "print(retained_features)\n",
    "\n",
    "# subset df\n",
    "df_sub = df_train.loc[:, sorted(retained_features.index.to_list())]\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa49d3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data frame for features(X): \n",
      "      X1     X2     X3      X4   X5    X7\n",
      "0  0.86  588.0  294.0  147.00  7.0  0.40\n",
      "1  0.71  710.5  269.5  220.50  3.5  0.25\n",
      "2  0.62  808.5  367.5  220.50  3.5  0.40\n",
      "3  0.98  514.5  294.0  110.25  7.0  0.10\n",
      "4  0.98  514.5  294.0  110.25  7.0  0.25\n",
      "\n",
      "Dimension of X:  (614, 6)\n",
      "\n",
      "Data frame for target(Y): \n",
      "         Y1\n",
      "0    32.75\n",
      "1    12.57\n",
      "2    16.74\n",
      "3    24.28\n",
      "4    28.55\n",
      "..     ...\n",
      "609  19.13\n",
      "610  10.39\n",
      "611  15.36\n",
      "612   6.81\n",
      "613  41.30\n",
      "\n",
      "[614 rows x 1 columns]\n",
      "\n",
      "Dimension of Y:  (614, 1)\n",
      "\n",
      "X data type:  float64\n",
      "Y data type:  float64\n",
      "\n",
      "Data dimension: \n",
      "(614, 6)\n",
      "(154, 6)\n",
      "(614,)\n",
      "(154,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the selected features data frame into numpy arrays\n",
    "X_train, y_train = create_feature_target_arrays(df_sub, \"Y1\", [\"Y1\"])\n",
    "\n",
    "# Also subset the selected features for X_test\n",
    "X_test = X_test[:, [0, 1, 2, 3, 4, 6]]\n",
    "\n",
    "# Check dimensions of all data\n",
    "print(\"\\nData dimension: \")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cedec",
   "metadata": {},
   "source": [
    "### **Experiments**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d966e77",
   "metadata": {},
   "source": [
    "-**Experiment 1**. Create an optimal Polynomial Regression model and train it using SGD with optimal hyperparameters. For the SGD based model selection, see the following notebook: https://github.com/rhasanbd/Linear-Regression-Extensive-Adventure/blob/master/Linear%20Regression-5-Polynomial%20SGD%20Regressor%20Model%20Selection.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22457175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n",
      "\n",
      "Best Score (negative mean squared error): -4.586243\n",
      "\n",
      "Optimal Hyperparameter values:  {'poly__degree': 5, 'sgd__alpha': 0.01, 'sgd__eta0': 0.01, 'sgd__l1_ratio': 0.2, 'sgd__max_iter': 500}\n",
      "\n",
      "\n",
      "CPU times: user 1.82 s, sys: 171 ms, total: 1.99 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Model Selection\n",
    "# Create a Pipeline object\n",
    "sgd_pipeline = Pipeline([\n",
    "    # Bias should be excluded because by default SGDRegressor adds bias via \n",
    "    # the \"fit_intercept\" parameter\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('sgd', SGDRegressor(penalty='elasticnet')),\n",
    "])\n",
    "\n",
    "# Create a dictionary object with hyperparamters as keys and lists of corresponding values\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3, 4, 5],\n",
    "    'sgd__alpha': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'sgd__l1_ratio': [1, 0.7, 0.5, 0.2, 0],\n",
    "    'sgd__max_iter': [500, 1000],\n",
    "    'sgd__eta0': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a GridSearchCV object and perform hyperparameter tuning\n",
    "sgd = GridSearchCV(sgd_pipeline, param_grid, scoring='neg_mean_squared_error', \n",
    "                       cv=3, verbose=1, n_jobs=-1)\n",
    "    \n",
    "def run_fit_without_warning():\n",
    "    if not sys.warnoptions:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "        # The model is trained with optimal hyperparamters, thus it's the optimal model\n",
    "        sgd.fit(X_train, y_train)\n",
    "        return sgd\n",
    "\n",
    "sgd = run_fit_without_warning()\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "params_optimal_sgd = sgd.best_params_\n",
    "\n",
    "print(\"\\nBest Score (negative mean squared error): %f\" % sgd.best_score_)\n",
    "print(\"\\nOptimal Hyperparameter values: \", params_optimal_sgd)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee518e9",
   "metadata": {},
   "source": [
    "- Report: degree of the optimal polynomial model and following results for both training and test data: Mean Squared Error & Coefficient of determination R2 variance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e01629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Mean squared error: 4.43\n",
      "Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.96\n",
      "\n",
      "Test: Mean squared error: 4.40\n",
      "Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "# Training data: Make prediction \n",
    "y_train_predicted_sgd = sgd.predict(X_train)\n",
    "\n",
    "print(\"Train: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "# Training data: Explained variance score: 1 is perfect prediction\n",
    "print(\"Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % r2_score(y_train, y_train_predicted_sgd))\n",
    "\n",
    "# Test data: Make prediction \n",
    "y_test_predicted = sgd.predict(X_test)\n",
    "\n",
    "print(\"\\nTest: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "# Training data: Explained variance score: 1 is perfect prediction\n",
    "print(\"Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652ce4e",
   "metadata": {},
   "source": [
    "- Create a learning curve (negative MSE vs train/validation data) using the optimal model. See how this can be done from block 14 of the following notebook: https://github.com/rhasanbd/Linear-Regression-Extensive-Adventure/blob/master/Linear%20Regression-2-OLS%20Polynomial%20Regression-Frequentist%20Approach.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96435a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Original Features:  6\n",
      "No. of Augmented Features:  461\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curve\n",
    "\n",
    "# Variable that specifies the degree of the polynomial to be added\n",
    "# to the feature vector\n",
    "poly_degree = params_optimal_sgd['poly__degree'] \n",
    "\n",
    "\n",
    "# Add polynomial and bias term with the feature vector \n",
    "# using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "\n",
    "print(\"No. of Original Features: \", X_train.shape[1])\n",
    "print(\"No. of Augmented Features: \", X_train_poly.shape[1])\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_poly)\n",
    "X_train_poly_standardized = scaler.transform(X_train_poly)\n",
    "\n",
    "\n",
    "# get optimal hyperparameters\n",
    "alpha = params_optimal_sgd['sgd__alpha']\n",
    "l1_ratio = params_optimal_sgd['sgd__l1_ratio']\n",
    "max_iter = params_optimal_sgd['sgd__max_iter']\n",
    "eta0 = params_optimal_sgd['sgd__eta0']\n",
    "\n",
    "\n",
    "# Create a learning curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(SGDRegressor(penalty='elasticnet', alpha=alpha, \n",
    "                                                                   l1_ratio=l1_ratio, max_iter=max_iter, \n",
    "                                                                    eta0=eta0), \n",
    "                                                       X_train_poly_standardized, \n",
    "                                                       y_train, cv=5, scoring='neg_mean_squared_error', \n",
    "                                                       train_sizes=np.linspace(0.01, 1.0, 50),\n",
    "                                                       n_jobs=-1)\n",
    "\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of validation set scores\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, \"r-+\", linewidth=3, label=\"Training score\")\n",
    "plt.plot(train_sizes, val_mean, \"b-\", linewidth=2, label=\"Cross-validation Score\")\n",
    "plt.legend(loc=\"best\", fontsize=14)\n",
    "plt.xlabel(\"Training set size\", fontsize=14)\n",
    "plt.ylabel(\"Negative MSE\", fontsize=14)\n",
    "plt.title(\"Learning Curve: Polynomial Degree = %d\" % poly_degree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d65e1",
   "metadata": {},
   "source": [
    "Answer the following question.\n",
    "- Q-1) What does the learning curve tell you? Is your model underfitting or overfitting? Does it have high/low bias and high/low variance? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e8732",
   "metadata": {},
   "source": [
    "In general, the model is as good as it can be - as can be seen from the high R2 of 0.96 for both training and testing data. The MSEs for both train and test data are similar at about 4.4, which points to a well-performed model.\n",
    "\n",
    "For the learning curve, the training data starts with relatively large negative RMSEs when few instances are used for training and with increasing number of training instances, first sharply, then gradually becoming closer to zero with the increasing number of instances and overcoming the noise, until eventually reaching a plateau of about -5 even when more data are added.\n",
    "\n",
    "Likewise, the validation data shows a similar trend as the training data, albeit starting with smaller negative RMSEs, and also reaches a plateau of about -5 as well like the training curve.\n",
    "\n",
    "In short, the training and validation curves converge and reach a plateau at about -5 negative RMSE with increasing number of training instances. The model is still very slightly underfitting, therefore having a high bias, because the negative RMSEs for both training and testing data are not zero, but it is the best complex model for a SGD linear regression solution, for this particular data set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
