{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ed3f61",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "Name        : Chia Sin Liew  \n",
    "Last edited : April 18th, 2022\n",
    "\n",
    "The goal of this assignment is to solve multi-class classification problems, visualization of image samples, and analyzing errors.\n",
    "- **Part A**: Multi-class Classification – Structured Data\n",
    "- **Part B**: Multi-class Classification – Unstructured Data & Analysis of Model\n",
    "Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b71ee",
   "metadata": {},
   "source": [
    "## Part A: Multi-class Classification - Structured Data\n",
    "\n",
    "You will perform multi-class classification on the following dataset using the Logistic Regression and KNN models.  \n",
    "\n",
    "Your goal is to: \n",
    "- Maximize test accuracy as well as precision, recall & F1 score for each class\n",
    "\n",
    "**Dataset**:\n",
    "The dataset, given in the *winequality-white.csv* file, is related to the white variants of the Portuguese \"Vinho Verde\"\" wine. It provides the physicochemical (inputs) and sensory (the output) variables. The dataset consists of characteristics of white wine (e.g., alcohol content, density, amount of citric acid, pH, etc.) with target variable \"quality\" representing rating of wine. The target variable \"quality\" ranges from 3 to 9. Higher rating indicates better quality of a wine. The classes are ordered and not balanced (e.g., there are much more normal wines than excellent or poor ones).\n",
    "\n",
    "Input features (based on physicochemical tests):\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- citric acid\n",
    "- residual sugar\n",
    "- chlorides\n",
    "- free sulfur dioxide\n",
    "- total sulfur dioxide\n",
    "- density\n",
    "- pH\n",
    "- sulphates\n",
    "- alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1b72a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys \n",
    "import numpy as np\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import SGDRegressor, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862ecc5",
   "metadata": {},
   "source": [
    "### Pre-processing:\n",
    "- Load the CSV file as a Pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ca8fe32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"winequality-white.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c76e2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a dictionary of column index to column name for easier\n",
    "# subsetting later in feature selection\n",
    "col_name_list = df.columns.to_list()\n",
    "col_name_dict = {elem : i for i, elem in enumerate(col_name_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794d8b1",
   "metadata": {},
   "source": [
    "- Create a data frame object for the features and another data frame object for the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e66169ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data frame for features (X): \n",
      "    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides\n",
      "0            7.0              0.27         0.36            20.7      0.045\n",
      "1            6.3              0.30         0.34             1.6      0.049\n",
      "2            8.1              0.28         0.40             6.9      0.050\n",
      "3            7.2              0.23         0.32             8.5      0.058\n",
      "4            7.2              0.23         0.32             8.5      0.058\n",
      "5            8.1              0.28         0.40             6.9      0.050\n",
      "\n",
      "Dimension of X:  (4898, 11)\n",
      "\n",
      "Data frame for target(Y): \n",
      "       quality\n",
      "0           6\n",
      "1           6\n",
      "2           6\n",
      "3           6\n",
      "4           6\n",
      "...       ...\n",
      "4893        6\n",
      "4894        5\n",
      "4895        6\n",
      "4896        7\n",
      "4897        6\n",
      "\n",
      "[4898 rows x 1 columns]\n",
      "\n",
      "Dimension of Y:  (4898, 1)\n"
     ]
    }
   ],
   "source": [
    "# y for target\n",
    "y = df['quality'].to_frame()\n",
    "# X for features\n",
    "X = df.drop(columns=['quality'])\n",
    "\n",
    "print(\"\\nData frame for features (X): \\n\", X.iloc[:6, :5])\n",
    "print(\"\\nDimension of X: \", X.shape)\n",
    "print(\"\\nData frame for target(Y): \\n\", y)\n",
    "print(\"\\nDimension of Y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d5c7d",
   "metadata": {},
   "source": [
    "- Create a **bar plot** to display the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84fadcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Counts')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = df.quality.value_counts()\n",
    "label_counts\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha=0.9)\n",
    "\n",
    "plt.xlabel('Wine Quality', fontsize=12)\n",
    "plt.ylabel('Counts', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9764c1d",
   "metadata": {},
   "source": [
    "- Convert the above two DataFrame objects into two NumPy arrays (you may use the NumPy “asarray” function).\n",
    "- Convert the target array type into “int”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d743a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X data type:  float64\n",
      "Y data type:  int64\n",
      "\n",
      "Y data type (after conversion):  int64\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y).ravel()\n",
    "\n",
    "print(\"\\nX data type: \", X.dtype)\n",
    "print(\"Y data type: \", y.dtype)\n",
    "\n",
    "y = y.astype('int')\n",
    "\n",
    "print(\"\\nY data type (after conversion): \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e148b4a",
   "metadata": {},
   "source": [
    "- Partition the dataset into training & test subsets: 80% training & 20% test (you may use Scikit-Learn’s train_test_split() function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e00f80d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3918, 11)\n",
      "(980, 11)\n",
      "(3918,)\n",
      "(980,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d43d7",
   "metadata": {},
   "source": [
    "### Feature Selection:\n",
    "You need to use the optimal set of features for the Logistic Regression models, which may consist of all features or a subset. Note that dropping the poorly correlated features with the target may not necessarily improve the test performance. You need to determine the optimal feature set empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2419aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.036</td>\n",
       "      <td>23.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99228</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.047</td>\n",
       "      <td>38.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.99365</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.56</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.49</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.046</td>\n",
       "      <td>87.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.99840</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.039</td>\n",
       "      <td>34.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.98946</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.7</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.033</td>\n",
       "      <td>65.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.99043</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.56</td>\n",
       "      <td>12.3</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.037</td>\n",
       "      <td>55.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.99120</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>11.1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3914</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.048</td>\n",
       "      <td>58.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.99220</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.30</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.057</td>\n",
       "      <td>50.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3916</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036</td>\n",
       "      <td>29.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.99080</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.32</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031</td>\n",
       "      <td>41.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.99234</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.26</td>\n",
       "      <td>12.3</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3918 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               6.1             0.410         0.04             1.3      0.036   \n",
       "1               8.6             0.170         0.28             2.7      0.047   \n",
       "2               7.6             0.360         0.49            11.3      0.046   \n",
       "3               6.4             0.310         0.28             2.5      0.039   \n",
       "4               5.8             0.200         0.24             1.4      0.033   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "3913            6.1             0.105         0.31             1.3      0.037   \n",
       "3914            6.6             0.370         0.07             1.4      0.048   \n",
       "3915            6.9             0.160         0.30             9.6      0.057   \n",
       "3916            6.4             0.160         0.42             1.0      0.036   \n",
       "3917            6.1             0.240         0.32             9.0      0.031   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    23.0                 121.0  0.99228  3.24       0.61   \n",
       "1                    38.0                 150.0  0.99365  3.10       0.56   \n",
       "2                    87.0                 221.0  0.99840  3.01       0.43   \n",
       "3                    34.0                 137.0  0.98946  3.22       0.38   \n",
       "4                    65.0                 169.0  0.99043  3.59       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "3913                 55.0                 145.0  0.99120  3.41       0.41   \n",
       "3914                 58.0                 144.0  0.99220  3.17       0.38   \n",
       "3915                 50.0                 185.0  0.99780  3.39       0.38   \n",
       "3916                 29.0                 113.0  0.99080  3.18       0.52   \n",
       "3917                 41.0                 134.0  0.99234  3.25       0.26   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.9      6.0  \n",
       "1        10.8      6.0  \n",
       "2         9.2      5.0  \n",
       "3        12.7      6.0  \n",
       "4        12.3      7.0  \n",
       "...       ...      ...  \n",
       "3913     11.1      7.0  \n",
       "3914     10.0      5.0  \n",
       "3915      9.6      6.0  \n",
       "3916     11.0      6.0  \n",
       "3917     12.3      7.0  \n",
       "\n",
       "[3918 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from X_train\n",
    "df_train = pd.DataFrame(np.concatenate((X_train, np.reshape(y_train, (3918, 1))), axis=1), \n",
    "                       columns=list(col_name_dict.keys()))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d41e9fa9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality                 1.000000\n",
      "alcohol                 0.433971\n",
      "pH                      0.104504\n",
      "sulphates               0.058670\n",
      "free sulfur dioxide     0.024063\n",
      "citric acid             0.002812\n",
      "residual sugar         -0.086419\n",
      "fixed acidity          -0.116735\n",
      "total sulfur dioxide   -0.156923\n",
      "volatile acidity       -0.186526\n",
      "chlorides              -0.210236\n",
      "density                -0.295684\n",
      "Name: quality, dtype: float64\n",
      "\n",
      "Keep: \n",
      "\n",
      "quality                 1.000000\n",
      "alcohol                 0.433971\n",
      "pH                      0.104504\n",
      "sulphates               0.058670\n",
      "free sulfur dioxide     0.024063\n",
      "citric acid             0.002812\n",
      "residual sugar         -0.086419\n",
      "fixed acidity          -0.116735\n",
      "total sulfur dioxide   -0.156923\n",
      "volatile acidity       -0.186526\n",
      "chlorides              -0.210236\n",
      "density                -0.295684\n",
      "Name: quality, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.61</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.99228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.56</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.99365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>11.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.99840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.98946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.56</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.8</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.99043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.99120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3914</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.38</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.4</td>\n",
       "      <td>6.6</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.99220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>6.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.99780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3916</th>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.52</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.99080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>7.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.26</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.99234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3918 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      quality  alcohol    pH  sulphates  free sulfur dioxide  citric acid  \\\n",
       "0         6.0      9.9  3.24       0.61                 23.0         0.04   \n",
       "1         6.0     10.8  3.10       0.56                 38.0         0.28   \n",
       "2         5.0      9.2  3.01       0.43                 87.0         0.49   \n",
       "3         6.0     12.7  3.22       0.38                 34.0         0.28   \n",
       "4         7.0     12.3  3.59       0.56                 65.0         0.24   \n",
       "...       ...      ...   ...        ...                  ...          ...   \n",
       "3913      7.0     11.1  3.41       0.41                 55.0         0.31   \n",
       "3914      5.0     10.0  3.17       0.38                 58.0         0.07   \n",
       "3915      6.0      9.6  3.39       0.38                 50.0         0.30   \n",
       "3916      6.0     11.0  3.18       0.52                 29.0         0.42   \n",
       "3917      7.0     12.3  3.25       0.26                 41.0         0.32   \n",
       "\n",
       "      residual sugar  fixed acidity  total sulfur dioxide  volatile acidity  \\\n",
       "0                1.3            6.1                 121.0             0.410   \n",
       "1                2.7            8.6                 150.0             0.170   \n",
       "2               11.3            7.6                 221.0             0.360   \n",
       "3                2.5            6.4                 137.0             0.310   \n",
       "4                1.4            5.8                 169.0             0.200   \n",
       "...              ...            ...                   ...               ...   \n",
       "3913             1.3            6.1                 145.0             0.105   \n",
       "3914             1.4            6.6                 144.0             0.370   \n",
       "3915             9.6            6.9                 185.0             0.160   \n",
       "3916             1.0            6.4                 113.0             0.160   \n",
       "3917             9.0            6.1                 134.0             0.240   \n",
       "\n",
       "      chlorides  density  \n",
       "0         0.036  0.99228  \n",
       "1         0.047  0.99365  \n",
       "2         0.046  0.99840  \n",
       "3         0.039  0.98946  \n",
       "4         0.033  0.99043  \n",
       "...         ...      ...  \n",
       "3913      0.037  0.99120  \n",
       "3914      0.048  0.99220  \n",
       "3915      0.057  0.99780  \n",
       "3916      0.036  0.99080  \n",
       "3917      0.031  0.99234  \n",
       "\n",
       "[3918 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate Pearson's correlation\n",
    "corr_threshold = 0.00\n",
    "\n",
    "feature_corr = df_train.corr()[\"quality\"].sort_values(ascending=False)\n",
    "print(feature_corr)\n",
    "\n",
    "# Keep features above the selected threshold\n",
    "print(\"\\nKeep: \\n\")\n",
    "retained_features = feature_corr[feature_corr.abs() >= corr_threshold]\n",
    "print(retained_features)\n",
    "\n",
    "# subset df\n",
    "df_sub = df_train.loc[:, retained_features.index.to_list()]\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790bf06",
   "metadata": {},
   "source": [
    "- rewrangle the dataframe back to numpy arrays and subset X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f73cc167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data frame for features(X): \n",
      "    alcohol    pH  sulphates  free sulfur dioxide  citric acid\n",
      "0      9.9  3.24       0.61                 23.0         0.04\n",
      "1     10.8  3.10       0.56                 38.0         0.28\n",
      "2      9.2  3.01       0.43                 87.0         0.49\n",
      "3     12.7  3.22       0.38                 34.0         0.28\n",
      "4     12.3  3.59       0.56                 65.0         0.24\n",
      "5     12.1  3.35       0.47                 31.0         0.36\n",
      "\n",
      "Dimension of X:  (3918, 11)\n",
      "\n",
      "Data frame for target(Y): \n",
      "       quality\n",
      "0         6.0\n",
      "1         6.0\n",
      "2         5.0\n",
      "3         6.0\n",
      "4         7.0\n",
      "...       ...\n",
      "3913      7.0\n",
      "3914      5.0\n",
      "3915      6.0\n",
      "3916      6.0\n",
      "3917      7.0\n",
      "\n",
      "[3918 rows x 1 columns]\n",
      "\n",
      "Dimension of Y:  (3918, 1)\n",
      "\n",
      "X data type:  float64\n",
      "Y data type:  float64\n",
      "\n",
      "Y data type (after conversion):  int64\n",
      "\n",
      "Data dimension: \n",
      "(3918, 11)\n",
      "(980, 11)\n",
      "(3918,)\n",
      "(980,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the selected features data frame into numpy arrays\n",
    "\n",
    "# declare function to create feature and target arrays\n",
    "def create_feature_target_arrays(df, target, drop_columns):\n",
    "    # y for target\n",
    "    y = df[target].to_frame()\n",
    "    # X for features\n",
    "    X = df.drop(columns=drop_columns)\n",
    "\n",
    "    print(\"\\nData frame for features(X): \\n\", X.iloc[:6, :5])\n",
    "    print(\"\\nDimension of X: \", X.shape)\n",
    "    print(\"\\nData frame for target(Y): \\n\", y)\n",
    "    print(\"\\nDimension of Y: \", y.shape)\n",
    "\n",
    "    # convert into np arrays\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y).ravel()\n",
    "\n",
    "    print(\"\\nX data type: \", X.dtype)\n",
    "    print(\"Y data type: \", y.dtype)\n",
    "    \n",
    "    y = y.astype('int')\n",
    "\n",
    "    print(\"\\nY data type (after conversion): \", y.dtype)\n",
    "    \n",
    "    return(X, y)\n",
    "\n",
    "X_train, y_train = create_feature_target_arrays(df_sub, 'quality', ['quality'])\n",
    "\n",
    "\n",
    "# subset the selected features for X_test\n",
    "X_test = X_test[:, [col_name_dict[a] for a in df_sub.columns.to_list()[1:]]]\n",
    "\n",
    "# check dimension of all data\n",
    "print(\"\\nData dimension: \")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9d142",
   "metadata": {},
   "source": [
    "### Experiments:\n",
    "You will perform multi-class classification using the following models. If a model requires the data to be standardized, you must do so prior training. You must perform hyperparameter tuning. For experiment 3, use the Pipeline object that will augment features, standardize (if required), and create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bf8b7",
   "metadata": {},
   "source": [
    "##### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c471d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training and test sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d2f70",
   "metadata": {},
   "source": [
    "#### Experiment 1) K-Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be58e2",
   "metadata": {},
   "source": [
    "##### Model selection by hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2e5aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 686 candidates, totalling 3430 fits\n",
      "\n",
      "Best Score:  0.6574771287825475\n",
      "Optimal Hyperparameter Values:  {'n_neighbors': 25, 'p': 1, 'weights': 'distance'}\n",
      "\n",
      "\n",
      "CPU times: user 7.03 s, sys: 1.87 s, total: 8.9 s\n",
      "Wall time: 12min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all \n",
    "# combinations of the hyperparamter values\n",
    "param_grid = {\n",
    "    'n_neighbors' : np.arange(1, 50),\n",
    "    'p' : [1, 2, 10, 50, 100, 500, 1000],\n",
    "    'weights' : [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn_clf, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "print(\"\\nBest Score: \", knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_knn)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f80337",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbc9260a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=25, p=1, weights='distance')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the best model using the optimal hyperparameter values\n",
    "knn = KNeighborsClassifier(**params_optimal_knn)\n",
    "\n",
    "\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be480ba",
   "metadata": {},
   "source": [
    "##### 5. Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d25638c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy:  1.0\n",
      "\n",
      "Test Accuracy:  0.6826530612244898\n",
      "\n",
      "Test Confusion Matrix: \n",
      "[[  0   0   3   1   0   0]\n",
      " [  0   4  19  10   2   0]\n",
      " [  0   0 202  94   4   0]\n",
      " [  0   0  52 348  31   1]\n",
      " [  0   0   2  77 101   0]\n",
      " [  0   0   0   6   9  14]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       1.00      0.11      0.21        35\n",
      "           5       0.73      0.67      0.70       300\n",
      "           6       0.65      0.81      0.72       432\n",
      "           7       0.69      0.56      0.62       180\n",
      "           8       0.93      0.48      0.64        29\n",
      "\n",
      "    accuracy                           0.68       980\n",
      "   macro avg       0.67      0.44      0.48       980\n",
      "weighted avg       0.70      0.68      0.67       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation threshold used to filter features: \", corr_threshold)\n",
    "\n",
    "y_train_predicted = knn.predict(X_train)\n",
    "\n",
    "print(\"\\nTraining Accuracy: \", np.mean(y_train_predicted == y_train))\n",
    "\n",
    "print(\"\\nTest Accuracy: \", knn.score(X_test, y_test))\n",
    "\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794bee5",
   "metadata": {},
   "source": [
    "#### Experiment 2) Logistic Regression (use batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41cd2e",
   "metadata": {},
   "source": [
    "##### One-versus-All (OvA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aafd3a",
   "metadata": {},
   "source": [
    "###### Model selection by hyperparameter tuning (Logistic Regression OvA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12d96194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
      "Best Score (F1 score): 0.533180\n",
      "Optimal Hyperparameter Values:  {'C': 0.1, 'max_iter': 500, 'multi_class': 'ovr', 'solver': 'liblinear', 'tol': 0.001}\n",
      "\n",
      "\n",
      "CPU times: user 426 ms, sys: 75.8 ms, total: 501 ms\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "param_grid = {\n",
    "    'solver': ['liblinear', 'newton-cg', 'lbfgs'], \n",
    "    'multi_class' : ['ovr'],\n",
    "    'tol': [1e-3, 1e-4], \n",
    "    'max_iter':[500, 1000],\n",
    "    'C': [0.1, 0.5, 1, 10, 15, 50, 100]\n",
    "}\n",
    "\n",
    "lg_reg = LogisticRegression()\n",
    "\n",
    "lg_reg_cv = GridSearchCV(lg_reg, param_grid, scoring='f1_micro', cv=3, verbose=1, n_jobs=-1)\n",
    "lg_reg_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = lg_reg_cv.best_params_\n",
    "\n",
    "print(\"Best Score (F1 score): %f\" % lg_reg_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b28a2",
   "metadata": {},
   "source": [
    "###### Train the model (Logistic Regression OvA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99d5ed12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=500, multi_class='ovr', solver='liblinear',\n",
       "                   tol=0.001)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_reg_ova = LogisticRegression(**params_optimal)\n",
    "\n",
    "lg_reg_ova.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a62c15",
   "metadata": {},
   "source": [
    "######  Evaluate the model on Test Data (Logistic Regression OvA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf3bd539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [8]\n",
      "\n",
      "Training Accuracy:  0.5375191424196019\n",
      "\n",
      "Test Accuracy:  0.5183673469387755\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[  0   0   3   0   0   1]\n",
      " [  0   0  20  15   0   0]\n",
      " [  0   0 154 144   2   0]\n",
      " [  0   0  84 339   9   0]\n",
      " [  0   0   4 161  15   0]\n",
      " [  0   0   0  25   4   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        35\n",
      "           5       0.58      0.51      0.55       300\n",
      "           6       0.50      0.78      0.61       432\n",
      "           7       0.50      0.08      0.14       180\n",
      "           8       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.52       980\n",
      "   macro avg       0.26      0.23      0.22       980\n",
      "weighted avg       0.49      0.52      0.46       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation threshold used to filter features: \", corr_threshold)\n",
    "\n",
    "print(\"No. of Iterations:\",lg_reg_ova.n_iter_ )\n",
    "\n",
    "y_train_predicted = lg_reg_ova.predict(X_train)\n",
    "\n",
    "print(\"\\nTraining Accuracy: \", np.mean(y_train_predicted == y_train))\n",
    "\n",
    "y_test_predicted = lg_reg_ova.predict(X_test)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09c4d8",
   "metadata": {},
   "source": [
    "##### Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed935d",
   "metadata": {},
   "source": [
    "###### Model selection by hyperparameter tuning (Softmax Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f523b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
      "Best Score (f1 score): 0.533691\n",
      "Optimal Hyperparameter Values:  {'C': 0.1, 'max_iter': 500, 'multi_class': 'multinomial', 'solver': 'newton-cg', 'tol': 0.001}\n",
      "\n",
      "\n",
      "CPU times: user 1.46 s, sys: 487 ms, total: 1.94 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "param_grid_sm = {\n",
    "    'solver': ['newton-cg', 'lbfgs'],\n",
    "    'multi_class': ['multinomial'],\n",
    "    'tol': [1e-3, 1e-4],\n",
    "    'max_iter': [500, 1000, 2000],\n",
    "    'C': [0.1, 0.5, 1, 10, 15, 50, 100]\n",
    "}\n",
    "\n",
    "lg_reg_sm = LogisticRegression()\n",
    "\n",
    "lg_reg_sm_cv = GridSearchCV(lg_reg_sm, param_grid_sm, scoring='f1_micro', cv=3, verbose=1, n_jobs=-1)\n",
    "lg_reg_sm_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal_sm = lg_reg_sm_cv.best_params_\n",
    "\n",
    "print(\"Best Score (f1 score): %f\" % lg_reg_sm_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_sm)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce1f78",
   "metadata": {},
   "source": [
    "###### Train the model (Softmax Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6c49a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=500, multi_class='multinomial',\n",
       "                   solver='newton-cg', tol=0.001)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_softmax = LogisticRegression(**params_optimal_sm)\n",
    "\n",
    "log_reg_softmax.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb1673",
   "metadata": {},
   "source": [
    "###### Evaluate the model on test data (Softmax Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f590afa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation threshold used to filter features:  0.0\n",
      "\n",
      "No. of iteration:  [16]\n",
      "\n",
      "Training Accuracy:  0.541858090862685\n",
      "\n",
      "Test Accuracy:  0.5193877551020408\n",
      "\n",
      "Test Confusion Matrix: \n",
      "[[  0   0   3   0   0   1]\n",
      " [  0   2  20  13   0   0]\n",
      " [  0   0 151 147   2   0]\n",
      " [  1   0  83 327  21   0]\n",
      " [  0   0   6 145  29   0]\n",
      " [  0   0   3  18   8   0]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       1.00      0.06      0.11        35\n",
      "           5       0.57      0.50      0.53       300\n",
      "           6       0.50      0.76      0.60       432\n",
      "           7       0.48      0.16      0.24       180\n",
      "           8       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.52       980\n",
      "   macro avg       0.43      0.25      0.25       980\n",
      "weighted avg       0.52      0.52      0.48       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation threshold used to filter features: \", corr_threshold)\n",
    "\n",
    "print(\"\\nNo. of iteration: \", log_reg_softmax.n_iter_)\n",
    "\n",
    "y_train_predicted = log_reg_softmax.predict(X_train)\n",
    "\n",
    "print(\"\\nTraining Accuracy: \", np.mean(y_train_predicted == y_train))\n",
    "\n",
    "y_test_predicted = log_reg_softmax.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Accuracy: \", np.mean(y_test_predicted == y_test))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4d1fb",
   "metadata": {},
   "source": [
    "####  **Experiment 3)** Polynomial Logistic Regression (use batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9db4d5",
   "metadata": {},
   "source": [
    "##### Model selection by hyperparameter tuning (Softmax Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6ef04dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best Score (f1 score): 0.572486\n",
      "Optimal Hyperparameter Values:  {'log_reg__C': 0.1, 'log_reg__max_iter': 1000, 'log_reg__multi_class': 'multinomial', 'log_reg__solver': 'lbfgs', 'log_reg__tol': 0.001, 'poly__degree': 4}\n",
      "\n",
      "\n",
      "CPU times: user 1min 24s, sys: 10.1 s, total: 1min 34s\n",
      "Wall time: 5h 54min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a Pipeline object\n",
    "log_reg_pipeline = Pipeline([\n",
    "    # LogisticRegression() also has fit_intercept=True by default\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('log_reg', LogisticRegression()),\n",
    "])\n",
    "\n",
    "param_grid_sm = {\n",
    "#     'poly__degree': [1, 2, 3, 4, 5],\n",
    "    'poly__degree': [2, 3, 4], # <- for all features because of how long it takes to find the optimal parameters\n",
    "    'log_reg__solver': ['newton-cg', 'lbfgs'],\n",
    "    'log_reg__multi_class': ['multinomial'],\n",
    "    'log_reg__tol': [1e-3, 1e-4],\n",
    "    'log_reg__max_iter': [500, 1000],\n",
    "#     'log_reg__C': [0.1, 0.5, 1, 10, 15, 50, 100]\n",
    "#     'log_reg__C': [0.1, 0.5, 1, 50]\n",
    "    'log_reg__C': [0.1, 1, 50] # <- for all features because of how long it takes to find the optimal parameters\n",
    "}\n",
    "\n",
    "\n",
    "lg_reg_sm_cv = GridSearchCV(log_reg_pipeline, param_grid_sm, scoring='f1_micro', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "\n",
    "def run_fit_without_warning():\n",
    "    if not sys.warnoptions:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "        # The model is trained with optimal hyperparamters, thus it's the optimal model\n",
    "        lg_reg_sm_cv.fit(X_train, y_train)\n",
    "        return lg_reg_sm_cv\n",
    "    \n",
    "lg_reg_sm_cv = run_fit_without_warning()\n",
    "# lg_reg_sm_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal_sm = lg_reg_sm_cv.best_params_\n",
    "\n",
    "print(\"Best Score (f1 score): %f\" % lg_reg_sm_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_sm)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11736bef",
   "metadata": {},
   "source": [
    "###### Train the model (Softmax Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592bbf1",
   "metadata": {},
   "source": [
    "There is no need to train the model again, we can just use the **optimal model created above** for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7be0ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# log_reg_softmax = LogisticRegression(**params_optimal_sm)\n",
    "\n",
    "# log_reg_softmax = LogisticRegression()\n",
    "# log_reg_softmax.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f4347",
   "metadata": {},
   "source": [
    "###### Evaluate the model on test data (Softmax Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "011a1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation threshold used to filter features:  0.0\n",
      "\n",
      "Training Accuracy:  0.8409903011740684\n",
      "\n",
      "Test Accuracy:  0.6040816326530613\n",
      "\n",
      "Test Confusion Matrix: \n",
      "[[  1   0   2   1   0   0]\n",
      " [  1  12  14   7   1   0]\n",
      " [  0   9 182  96  12   1]\n",
      " [  0   3  84 293  47   5]\n",
      " [  0   1   9  71  92   7]\n",
      " [  0   0   1   6  10  12]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.50      0.25      0.33         4\n",
      "           4       0.48      0.34      0.40        35\n",
      "           5       0.62      0.61      0.61       300\n",
      "           6       0.62      0.68      0.65       432\n",
      "           7       0.57      0.51      0.54       180\n",
      "           8       0.48      0.41      0.44        29\n",
      "\n",
      "    accuracy                           0.60       980\n",
      "   macro avg       0.54      0.47      0.50       980\n",
      "weighted avg       0.60      0.60      0.60       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation threshold used to filter features: \", corr_threshold)\n",
    "\n",
    "# print(\"\\nNo. of iteration: \", lg_reg_sm_cv.n_iter_)\n",
    "\n",
    "y_train_predicted = lg_reg_sm_cv.predict(X_train)\n",
    "\n",
    "print(\"\\nTraining Accuracy: \", np.mean(y_train_predicted == y_train))\n",
    "\n",
    "y_test_predicted = lg_reg_sm_cv.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Accuracy: \", np.mean(y_test_predicted == y_test))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7ae75",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "- Optimal set of features in experiments 2 and 3. If you used all features, report that\n",
    "as well.\n",
    "- For each optimal model report training accuracy, test accuracy, test confusion\n",
    "matrix, weighted average of the test precision, recall, F1 scores, degree of the optimal polynomial model (experiments 3), and optimal values of the model hyperparameters. Use the following table in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b0511",
   "metadata": {},
   "source": [
    "Answer the following questions.\n",
    "- **Q-1)** Which model did you find most effective (optimal test performance)?\n",
    "Explain why this model performed better than other models in your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636a340",
   "metadata": {},
   "source": [
    "KNN, using all features, is the most effective according to its test performace (Train accuracy of 1, test accuracy of 0.68, weighted precision of 0.7, weighted recall of 0.68 and f1-score of 0.67. I am not sure why KNN performs the best, but my best guess is that it is the only non-parametric model among the models we tried and since the dataset is relatively small and an instance based model works better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5b1f6",
   "metadata": {},
   "source": [
    "- **Q-2)** Which model did you find most efficient (less time for hyperparameter\n",
    "tuning)? Explain why it is efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e257fe6",
   "metadata": {},
   "source": [
    "The most efficient model is Logistic Regression (Ova). It is the fastest because it is a linear regression model and there are comparatively less parameters to train compared to Logistic Regression (Softmax) and Polynomial Logistic Regression. It is also faster since it is model based instead of instance based like KNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
